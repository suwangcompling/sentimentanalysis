{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Continuous Bag-Of-Word Models (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. ONE-WORD CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown_words_raw = brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLEAN DATA\n",
    "def word_normalize(word):\n",
    "    return str(PorterStemmer().stem(word.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_words = [word_normalize(word) for word in brown_words_raw] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1: 1-CONTEXT + NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Word2Vec1WordNumpy:\n",
    "    \n",
    "    def __init__(self, X, N=10, lr=.001, verbose=True): \n",
    "        # Vocab, Dimensions, Learning Rate.\n",
    "        self.vocab = list(set(X)) # vocab.\n",
    "        self.V = len(self.vocab) # vocab size.\n",
    "        self.N = N # word-vec's size.\n",
    "        self.lr = lr # learning rate.\n",
    "        self.verbose = verbose\n",
    "        # Weights random initialization.\n",
    "        self.W1 = np.random.rand(self.V,self.N) # weights between one-hot and word-vecs.\n",
    "        self.h = np.random.rand(self.N) # word-vec, begotten by x^T * W1.\n",
    "        self.W2 = np.random.rand(self.N,self.V) # weights between word-vecs and output layer.\n",
    "    \n",
    "    def softmax(self, v):\n",
    "        Z = sum(np.exp(v_i) for v_i in v) # normalizing constant.\n",
    "        return [np.exp(v_i)/Z for v_i in v]\n",
    "    \n",
    "    def one_hot_transform(self, word): \n",
    "        one_hot = np.zeros(len(self.vocab))\n",
    "        if word in self.vocab:\n",
    "            one_hot[self.vocab.index(word)] = 1.\n",
    "        return one_hot\n",
    "    \n",
    "    def word2vec(self, word):\n",
    "        return np.dot(self.one_hot_transform(word), self.W1)\n",
    "    \n",
    "    def forward_propagation(self, word):\n",
    "        x = self.one_hot_transform(word)\n",
    "        self.h = np.dot(x, self.W1)\n",
    "        output = self.softmax(np.dot(self.h, self.W2))\n",
    "        return output\n",
    "    \n",
    "    def backward_propagation(self, word, next_word):\n",
    "        x, next_word = self.one_hot_transform(word), self.one_hot_transform(next_word)\n",
    "        next_word_hat = self.forward_propagation(word) # prediction.\n",
    "        # update W2 (note: word-vec -- W2 -- output).\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.V):\n",
    "                self.W2[i][j] -= self.lr * (next_word_hat[j]-next_word[j]) * self.h[i]\n",
    "        # update W1 (note: one-hot -- W1 -- word-vec).\n",
    "        EH = [ sum( (next_word_hat[j]-next_word[j])*self.W2[i][j] for j in range(self.V) ) for i in range(self.N)  ]\n",
    "        for j in range(self.V):\n",
    "            self.W1[j] -= self.lr * np.array(EH)\n",
    "    \n",
    "    def train(self, X_train, Y_train, repeat=1): # X: a list of words; Y: a list of next-words of X.\n",
    "        counter = 0\n",
    "        for _ in range(repeat):\n",
    "            for word,next_word in zip(X_train,Y_train):\n",
    "                counter +=1\n",
    "                self.backward_propagation(word, next_word)\n",
    "                if self.verbose and counter%100==0: print \"Trained %d words.\" % counter\n",
    "    \n",
    "    def predict(self, word, verbose=True):\n",
    "        next_word_hat = self.vocab[np.argmax(self.forward_propagation(word))]\n",
    "        print \"The predicted next-word for '%s' is '%s'\" % (word,next_word_hat)\n",
    "        return next_word_hat\n",
    "        \n",
    "    def evaluate(self, X_test, Y_test, verbose=True):\n",
    "        counter = 0\n",
    "        correct = 0\n",
    "        for word,next_word in zip(X_test,Y_test):\n",
    "            next_word_hat = self.predict(word, verbose)\n",
    "            counter += 1\n",
    "            print next_word, next_word_hat\n",
    "            correct += (next_word==next_word_hat)\n",
    "            if verbose: print \"Correct\" if next_word==next_word_hat else \"Wrong\"\n",
    "        print correct                                \n",
    "    \n",
    "# NB: ADDING MAY IMPROVE PERFORMANCE.\n",
    "# NB: DIFFERENT BIAS TERMS FOR W1 & W2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ['this','is','extremely','good']\n",
    "Y = ['is', 'extremely', 'good']\n",
    "N = 3\n",
    "mdl = Word2Vec1WordNumpy(X, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  ['this', 'is', 'good', 'extremely']\n",
      "Correct Next Word for 'this' is 'is'.\n",
      "Correct Next Word for 'is' is 'good'.\n",
      "Correct Next Word for 'good' is 'extremely'.\n"
     ]
    }
   ],
   "source": [
    "print \"Vocab: \", mdl.vocab\n",
    "for i in range(len(mdl.vocab)-1):\n",
    "    print \"Correct Next Word for '%s' is '%s'.\" % (mdl.vocab[i], mdl.vocab[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next-word for 'this' is 'this'\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "# BEFORE-TRAINING: RANDOM WEIGHTS\n",
    "print mdl.predict('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 s, sys: 8.15 ms, total: 1.9 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10000):\n",
    "    for w,nxt in zip(X,next_words):\n",
    "        mdl.backward_propagation(w,nxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next-word for 'this' is 'is'\n",
      "The predicted next-word for 'is' is 'extremely'\n",
      "The predicted next-word for 'extremely' is 'good'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREDICTING NEXT WORDS\n",
    "#  NB: the model is not expected to know the next word of 'good', as it's not trained for that.\n",
    "mdl.predict('this')\n",
    "mdl.predict('is')\n",
    "mdl.predict('extremely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98455355  1.36145238  1.09804509]\n",
      "[ 1.03038062  0.69486647  1.75002165]\n",
      "[ 0.47375052  1.45647869  1.28320007]\n",
      "[ 1.1462976   0.99098018  1.15359568]\n"
     ]
    }
   ],
   "source": [
    "# LEARN WORD VECTORS\n",
    "#  NB: this will only be useful when the training is large, \n",
    "#   we will then have semantically comparable to inspect.\n",
    "print mdl.word2vec('this')\n",
    "print mdl.word2vec('is')\n",
    "print mdl.word2vec('extremely')\n",
    "print mdl.word2vec('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Brown News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fulton', 'counti', 'grand', 'juri', 'said', 'friday', 'an', 'investig', 'of']\n",
      "['fulton', 'counti', 'grand', 'juri', 'said', 'friday', 'an', 'investig', 'of', \"atlanta'\"]\n"
     ]
    }
   ],
   "source": [
    "# CREAT TRAINING & TESTING\n",
    "X_train, Y_train = brown_words[:1000], brown_words[1:1001]\n",
    "print X_train[:10]\n",
    "print Y_train[:10]\n",
    "X_test, Y_test = brown_words[2001:2101], brown_words[2002:2102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 100 words.\n",
      "Trained 200 words.\n",
      "Trained 300 words.\n",
      "Trained 400 words.\n",
      "Trained 500 words.\n",
      "Trained 600 words.\n",
      "Trained 700 words.\n",
      "Trained 800 words.\n",
      "Trained 900 words.\n",
      "Trained 1000 words.\n",
      "Trained 1100 words.\n",
      "Trained 1200 words.\n",
      "Trained 1300 words.\n",
      "Trained 1400 words.\n",
      "Trained 1500 words.\n",
      "Trained 1600 words.\n",
      "Trained 1700 words.\n",
      "Trained 1800 words.\n",
      "Trained 1900 words.\n",
      "Trained 2000 words.\n",
      "Trained 2100 words.\n",
      "Trained 2200 words.\n",
      "Trained 2300 words.\n",
      "Trained 2400 words.\n",
      "Trained 2500 words.\n",
      "Trained 2600 words.\n",
      "Trained 2700 words.\n",
      "Trained 2800 words.\n",
      "Trained 2900 words.\n",
      "Trained 3000 words.\n",
      "Trained 3100 words.\n",
      "Trained 3200 words.\n",
      "Trained 3300 words.\n",
      "Trained 3400 words.\n",
      "Trained 3500 words.\n",
      "Trained 3600 words.\n",
      "Trained 3700 words.\n",
      "Trained 3800 words.\n",
      "Trained 3900 words.\n",
      "Trained 4000 words.\n",
      "Trained 4100 words.\n",
      "Trained 4200 words.\n",
      "Trained 4300 words.\n",
      "Trained 4400 words.\n",
      "Trained 4500 words.\n",
      "Trained 4600 words.\n",
      "Trained 4700 words.\n",
      "Trained 4800 words.\n",
      "Trained 4900 words.\n",
      "Trained 5000 words.\n",
      "CPU times: user 25.1 s, sys: 252 ms, total: 25.3 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAINING\n",
    "mdl = Word2Vec1WordNumpy(X_train,5) # 5D word vectors.\n",
    "mdl.train(X_train, Y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_vec = mdl.word2vec('thi')\n",
    "that_vec = mdl.word2vec('that')\n",
    "doctor_vec = mdl.word2vec('doctor')\n",
    "fulton_vec = mdl.word2vec('fulton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898138339613\n",
      "0.286958743578\n",
      "0.111600231987\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print scipy.spatial.distance.cosine(this_vec, that_vec)\n",
    "print scipy.spatial.distance.cosine(this_vec, doctor_vec)\n",
    "print scipy.spatial.distance.cosine(this_vec, fulton_vec)\n",
    "    # NB: this is intuitive: \n",
    "    #  - 'this' and 'that' frequently appear in similar contexts.\n",
    "    #  - 'this' frequently modifies 'doctor' (thus appear together).\n",
    "    #  - 'this' almost never appear with 'fulton'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
